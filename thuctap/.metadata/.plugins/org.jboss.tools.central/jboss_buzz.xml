<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Using Dashbuilder with Google Spreadsheets</title><link rel="alternate" href="https://blog.kie.org/2023/04/using-dashbuilder-with-google-spreadsheets.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2023/04/using-dashbuilder-with-google-spreadsheets.html</id><updated>2023-04-26T17:04:45Z</updated><content type="html">can read from JSON and CSV contents. The only requirement is to make the file accessible to the machine where Dashbuilder is running. The issues you may face are: * : To solve CORS issue you need to use a web proxy to access the service or correctly configure CORS headers; * : To solve this you can have a backend with authentication setup or include headers in the dataset declaration - uuid: my_dataset   url: http://acme.org/myfile.csv       headers:   Authorization: Bearer {token} For Google spreadsheets we don’t have to worry about CORS or authentication as long as the document is published on the internet.  STEPS TO READ A GOOGLE SPREADSHEET  FROM DASHBUILDER * Publish the document on the internet: The document must be public on the internet, so make it sure that you publish it with the option Anyone with the link. Here’s how it looks like in Portuguese: * Now you need the sheet ID. It is what will allow us to get the same sheet output in CSV format. In our example here’s the ID: * Use the ID on this URL template. You can also specify a sheet name if you want In our example here’s the final URL: * Now you can use the sheet from Dashbuilder. Just remember that for CSVs the first row is skipped. Here’s a sample dashboard which you can use to get started: properties: sheet_id: 1XuyPTyrjMFXQ1ey6Bg9AEcrpwZ60CnLQVEs4-DEDrcc datasets:    - uuid: sheet      url: https://docs.google.com/spreadsheets/d/${sheet_id}/gviz/tq?tqx=out:csv pages:    - components:          - settings:                type: BARCHART                lookup:                    uuid: sheet    You can edit this same example using our . CONCLUSION Dashbuilder can easily integrate with Google Spreadsheet and other documents available on the internet. Stay tuned for more Dashbuilder tutorials and articles! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Quarkus 3.0, our new major release, is here!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-0-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-0-final-released/</id><updated>2023-04-26T00:00:00Z</updated><published>2023-04-26T00:00:00Z</published><summary type="html">Quarkus 3.0 is the result of a lot of work and dedication from the community. Quarkus continues to be an Open Source stack to write Java applications offering unparalleled startup time, memory footprint, and developer experience. The development of Quarkus 3 started last year on March 18, 2022, with the...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-04-26T00:00:00Z</dc:date></entry><entry><title>Optimize container images for NGINX and Apache HTTPd</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/25/optimize-container-images-nginx-and-apache-httpd" /><author><name>Honza Horak</name></author><id>40488572-2eb5-4d73-bbf3-95f8210de674</id><updated>2023-04-25T07:00:00Z</updated><published>2023-04-25T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/containers/"&gt;Container&lt;/a&gt; image size matters. Let’s look at an experiment that reduced Apache HTTP and NGINX servers to micro container images. This article walks through the process we used to achieve the final result, plus how many megabytes (MB) this approach saved.&lt;/p&gt; &lt;p&gt;For this experiment, we used Fedora RPMs, but a similar approach should work in other operating systems or container images, as you see in the list of available images (there is an Apache HTTP server image that uses CentOS Stream 8 and 9 RPMs).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A shortcut:&lt;/strong&gt; If you are only interested in the result and want to try out the micro variant of Apache HTTP or NGINX server container images, check out the images in the following registries.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/fedora/httpd-24-micro&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/fedora/nginx-122-micro&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/sclorg/httpd-24-micro-c8s&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/sclorg/httpd-24-micro-c9s&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The benefits of smaller container images&lt;/h2&gt; &lt;p&gt;First, let’s explain a bit more about the story behind those micro containers.&lt;/p&gt; &lt;p&gt;Container images include everything that a specific application needs except a &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; kernel. That's the spirit of container technology. Here, our focus is on web servers, so in addition to the Apache HTTPd and NGINX server daemons, the container needs to include also libraries that those daemons use, necessary userspace components, etc.&lt;/p&gt; &lt;p&gt;Even in the year 2023 when Internet speeds are tremendous, the size of such containers is important. For example, it matters in an environment where the Internet speed is still very limited (have you heard about &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt; running on satellites somewhere far in space?). It can help make the user experience more delightful (waiting dozens of seconds is not fun) or limit potential attack vectors because of unnecessary pieces of software in the container image that could work without them.&lt;/p&gt; &lt;p&gt;These are just a few reasons why developers want to make the container image as small as practically possible. Now let's review the steps we took to reduce the web server container image size to a minimum. Our experiments used Apache HTTP server 2.4 and NGINX server 1.22.&lt;/p&gt; &lt;h2&gt;Choosing binaries&lt;/h2&gt; &lt;p&gt;For these tech preview container images, we decided to use a Fedora 36 base image and therefore take RPMs from Fedora repositories. There are different attempts to make the container image small by compiling just the necessary pieces directly from the source, which results in a small image, but that’s not always a good idea.&lt;/p&gt; &lt;p&gt;Using packages from a distribution has a clear benefit—they are well-tested, maintained when there is a security issue, interact well with the rest of the operating system, and are proven to work well outside of the container, so we only need to focus on the container specifics.&lt;/p&gt; &lt;p&gt;You might think about removing files once they are installed as RPMs; this might make the image smaller, but it would be rather risky and a container image could crash in some corner cases when some files would be needed, despite it seemed not like that. If our goal is to create a container image good enough for production, we should follow a simple principle: to not remove files from RPMs, so RPM packages are installed fully or not at all.&lt;/p&gt; &lt;p&gt;However, let's first see what we started with. The container images users currently can use for the latest stable web servers are as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Container image        | Compressed size | Uncompressed size | Apache HTTP Server 2.4 | 120 MB          | 376 MB            | Nginx 1.22             | 111 MB          | 348 MB            |&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Minimizing the container image&lt;/h2&gt; &lt;p&gt;The main trick is to use a two-phase building of the container image. That means that we use a parent image only for installing RPMs to an empty directory and then use only the content of this directly as a final result. This way we not only get rid of the package installer (DNF) but also the RPM database and RPM tooling itself. We end up with only the web server RPMs and their direct and indirect dependencies.&lt;/p&gt; &lt;p&gt;This change alone already makes a big difference in size, but it also means installing additional software into such an image is not easy. Extending such an image would either mean copying files directly to the image, or the image would need to be rebuilt from scratch. That’s an acceptable disadvantage because, for many use cases, users do not need to extend images with web servers.&lt;/p&gt; &lt;h3&gt;Analyzing dependencies&lt;/h3&gt; &lt;p&gt;The next step was looking closely at what we actually have in the container image. For example, we see &lt;a href="https://developers.redhat.com/cheat-sheets/systemd-commands-cheat-sheet"&gt;systemd&lt;/a&gt; and all of its dependencies. That makes sense when we install the web servers outside of the container image, but in the container? It's likely not needed. So, we worked with Apache HTTPd and NGINX server maintainers, who helped us to get rid of the systemd dependency by installing only &lt;code&gt;httpd-core&lt;/code&gt; and &lt;code&gt;nginx-core&lt;/code&gt; packages. We also avoided installing the Perl module in the case of NGINX, because it pulled in a lot of additional MBs in form of the Perl interpreter and several base libraries.&lt;/p&gt; &lt;p&gt;These changes again helped to squeeze the size significantly. We didn't stop there, though. We analyzed other packages and saw that we installed &lt;code&gt;nss_wrapper&lt;/code&gt; that pulled in the Perl interpreter as well. We also installed &lt;code&gt;gettext&lt;/code&gt; package in order to have &lt;code&gt;envsubst&lt;/code&gt; utility available (for expanding Bash variables in configuration files, as environment variables are common ways to configure container images). In both cases, we worked with the package maintainers, and they allowed us to use only minimal required parts of their tools so we could only install &lt;code&gt;nss_wrapper-libs&lt;/code&gt; and &lt;code&gt;envsubst&lt;/code&gt; packages, which removed additional MBs.&lt;/p&gt; &lt;h3&gt;What we kept in the image&lt;/h3&gt; &lt;p&gt;What we didn't get rid of were several Bash scripts that help the container when starting (starting the daemon, handling the configuration, etc.). These scripts do not take more than a few kilobytes (kB) anyway, so we didn’t touch those.&lt;/p&gt; &lt;p&gt;There are also a couple of other packages that we installed explicitly to make the container images work reasonably (&lt;code&gt;coreutils-single&lt;/code&gt;, &lt;code&gt;glibc-minimal-langpack&lt;/code&gt;), but those were already made as minimal as possible.&lt;/p&gt; &lt;h2&gt;Using the micro web server images&lt;/h2&gt; &lt;p&gt;The container images we worked with are designed to be used either directly via the container command-line interface (&lt;a href="https://developers.redhat.com/articles/podman-next-generation-linux-container-tools/"&gt;Podman&lt;/a&gt; or Docker) in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, but they were specifically designed to work well in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Read more about specific usage in the README files available in the GitHub repositories:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/sclorg/httpd-container/blob/master/2.4-micro/root/usr/share/container-scripts/httpd/README.md"&gt;httpd-container&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/sclorg/nginx-container/blob/master/1.22-micro/README.md"&gt;nginx-container&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The final result&lt;/h2&gt; &lt;p&gt;Did we succeed? Except for the Perl module in the case of the NGINX container image, the tests we have for the images passed fine for the micro container images as well. So, the main use cases should work fine and the micro images should still be pretty useful.&lt;/p&gt; &lt;p&gt;Now we can see how big the micro images are after all those changes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Container image              | Compressed size | Uncompressed size | Apache HTTP Server 2.4 micro | 16 MB (13%)     | 46 MB (12%)       | Nginx 1.22 micro             | 23 MB (21%)     | 63 MB (18%)       |&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In summary, we were able to decrease to approximately one-fifth of the original size, so the images will be downloaded five times faster and consume less than one-fifth of space.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The price for such a great difference is not large; the most important feature we lose is the ability to install additional software (due to the missing RPM and DNF). If your use case is to serve static content, then micro HTTPd and NGINX images should do the work without trouble. If your use case is beyond this and you want to serve something complicated or install further RPMs, then the original web server images might be a better choice for you. Or you can create your own micro image, based on the principles explained in this article.&lt;/p&gt; &lt;p&gt;Enjoy the micro web servers, and don't forget to let us know what you think by visiting the GitHub projects below. You can also leave a comment here if you simply like this approach and the images work for you.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/sclorg/nginx-container/issues"&gt;nginx-container&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/sclorg/httpd-container/issues"&gt;httpd-container&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Looking for more? Explore &lt;a href="https://developers.redhat.com/topics/containers/all"&gt;other container tutorials&lt;/a&gt; from Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/25/optimize-container-images-nginx-and-apache-httpd" title="Optimize container images for NGINX and Apache HTTPd"&gt;Optimize container images for NGINX and Apache HTTPd&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Honza Horak</dc:creator><dc:date>2023-04-25T07:00:00Z</dc:date></entry><entry><title>How to debug OpenShift operators on a live cluster using dlv</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/24/how-debug-openshift-operators-live-cluster-using-dlv" /><author><name>Swarup Ghosh</name></author><id>1c9b4d81-6214-4d43-99f4-82e55760ed12</id><updated>2023-04-24T18:00:00Z</updated><published>2023-04-24T18:00:00Z</published><summary type="html">&lt;p&gt;Debugging operators can be tricky, especially if the operator needs to be debugged on a live cluster, which is useful for developing &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; cluster operators. Remotely running delve debugger inside the &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; helps in this case. This article is about debugging operators live in an OpenShift cluster on the fly by rebuilding the operator container image and using &lt;code&gt;go dlv&lt;/code&gt; remotely through the &lt;code&gt;oc&lt;/code&gt; port-forward.&lt;/p&gt; &lt;h2&gt;About cluster operators and Delve debugger&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes/operators/"&gt;Kubernetes operators&lt;/a&gt; are used to manage the lifecycle of applications within a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster. The operator pattern is aimed at simplifying installation, management, and configuration of applications and services. OpenShift is an operator-first platform with its fundamental architecture strongly rooted to various operators. In the OpenShift world, operators help to manage the lifecycle of the running cluster as well as different applications that run on top of it. With each OpenShift cluster installation, there comes a set of default operators known as &lt;a href="https://docs.openshift.com/container-platform/4.12/operators/operator-reference.html" target="_blank"&gt;cluster operators&lt;/a&gt; which help to manage different aspects of the OpenShift cluster. An OpenShift cluster marks cluster creation as complete once all the cluster operators running in the cluster can reach a healthy running state.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/openshift/cluster-version-operator" target="_blank"&gt;Cluster Version Operator&lt;/a&gt; (CVO) is one of the important cluster operators that reconciles the resources within the cluster to match them to their desired state while ensuring that other cluster operators remain healthy. Each cluster operator manages specific area of the cluster’s functionality and these operator deployments observe a few set of args in their respective deployments manifests as per the configuration set by the cluster apart from other necessary values.&lt;/p&gt; &lt;p&gt;For the purpose of this example, we will use the &lt;a href="https://github.com/openshift/cluster-kube-apiserver-operator/" target="_blank"&gt;cluster-kube-apiserver-operator&lt;/a&gt; running on an OpenShift cluster and live debug the running operator remotely on a VS Code setup using go dlv debugger.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/go-delve/delve" target="_blank"&gt;Delve&lt;/a&gt; is one of the most widely used debuggers used for &lt;a href="https://developers.redhat.com/topics/go"&gt;Golang&lt;/a&gt;. It has the option to allow debugging a go binary remotely through a connected tcp port with the help of which developers can get debug access to the operator binary running inside the actual cluster.&lt;/p&gt; &lt;h2&gt;Debugging tutorial steps&lt;/h2&gt; &lt;p&gt;The following tutorial is aimed at allowing developers to debug operators running on the cluster.&lt;/p&gt; &lt;p&gt;The first step to modifying any cluster operator running on OpenShift is to disable the CVO this would help prevent the cluster operator deployment manifests to be tweaked without having it be reconciled to to the default image. With the kubeconfig of the running cluster and via &lt;code&gt;oc&lt;/code&gt; command, the following command would disable CVO completely.&lt;/p&gt; &lt;pre&gt; &lt;code class="bash language-bash"&gt;$ oc scale --replicas=0 deploy/cluster-version-operator -n openshift-cluster-version &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, if the cluster operator itself allows the user to set it to an unmanaged state through the ClusterVersion object, for the kube-api-server operator it would be as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash hljs"&gt;$ oc patch clusterversion/version --&lt;span class="hljs-built_in"&gt;type&lt;/span&gt;=&lt;span class="hljs-string"&gt;'merge'&lt;/span&gt; -p &lt;span class="hljs-string"&gt;"&lt;span class="hljs-subst"&gt;$(cat &lt;&lt;- EOF spec: overrides: - group: apps kind: Deployment name: kube-apiserver-operator namespace: openshift-kube-apiserver-operator unmanaged: true EOF )&lt;/span&gt;"&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Either methods should work except that the first method completely disables CVO while the second method is specific to allowing deployment changes to kube-api-server operator only. It is noteworthy to mention that these steps are not required if you plan to use this tutorial to debug operators which are not OpenShift cluster operators. In that case, you can start past this point.&lt;/p&gt; &lt;p&gt;The deployment for kube-api-server operator can be displayed as follows:&lt;/p&gt; &lt;pre class="part" data-endline="65" data-position="3499" data-startline="36"&gt; &lt;code class="bash hljs"&gt;$ oc get deployment/kube-apiserver-operator -o yaml -n openshift-kube-apiserver-operator --- name: kube-apiserver-operator namespace: openshift-kube-apiserver-operator ownerReferences: - apiVersion: config.openshift.io/v1 kind: ClusterVersion name: version uid: 4b0f3c33-ade3-4e67-832f-169f8e297639 --- spec: --- spec: automountServiceAccountToken: &lt;span class="hljs-literal"&gt;false&lt;/span&gt; containers: - args: - --config=/var/run/configmaps/config/config.yaml &lt;span class="hljs-built_in"&gt;command&lt;/span&gt;: - cluster-kube-apiserver-operator - operator &lt;span class="hljs-built_in"&gt;env&lt;/span&gt;: - name: IMAGE value: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0314d2c0df2cf572cf8cfd13212c04dff8ef684f1cdbb93e22027c86852f1954 - name: OPERATOR_IMAGE value: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2766c4f3330423b642ad82eaa5df66a5a46893594a685fd0562a6460a2719187 - name: OPERAND_IMAGE_VERSION value: 1.25.2 --- &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As one can observe for the proper functioning of this operator, there are various container args plus environment variables required to be set as a part of the container spec. While debugging operators, developers usually start off by running the Golang binaries locally but it can get cumbersome to set these args and environment variables manually. To avoid this, we can rebuild the operator’s image with a debugger-friendly binary, push it to a registry, and use it in the deployment manifest of the operator.&lt;/p&gt; &lt;p&gt;In the case of kube-apiserver-operator, one can obtain the source of the operator by cloning it from &lt;a href="https://github.com/openshift/cluster-kube-apiserver-operator" target="_blank"&gt;the GitHub repository&lt;/a&gt;&lt;span data-position="5171" data-size="154"&gt;, and this step should be similar for any other operator or the developer should have the source files ready on their local at the time of reading this.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Launch a VS Code editor from the local folder containing the source files. Make the following changes to the Dockerfile of the operator. &lt;/p&gt; &lt;p&gt;We create a copy of the Dockerfile and ensure that Go build of the binaries are built with the gcflags as &lt;code&gt;“all=-N -l”&lt;/code&gt; before. It can either be passed at command line using &lt;code&gt;go build -gcflags="all=-N -l"&lt;/code&gt; or by setting environment variable &lt;code&gt;GCFLAGS&lt;/code&gt;. This should be set as a builder stage of the Dockerfile where binaries are compiled from source. For kube-apiserver-operator, the environment variable was set.&lt;/p&gt; &lt;pre class="part" data-endline="84" data-position="5916" data-startline="75"&gt; &lt;code class="dockerfile hljs"&gt;&lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... AS builder &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; go install -mod=&lt;span class="hljs-built_in"&gt;readonly&lt;/span&gt; github.com/go-delve/delve/cmd/dlv@latest&lt;/span&gt; ... &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; . .&lt;/span&gt; &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GO_PACKAGE github.com/openshift/cluster-kube-apiserver-operator &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GCFLAGS &lt;span class="hljs-string"&gt;"all=-N -l"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; make build --warn-undefined-variables&lt;/span&gt; ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The same dlv binary needs to be copied over to the final image as well, using the following command:&lt;/p&gt; &lt;pre class="part" data-endline="92" data-position="6256" data-startline="88"&gt; &lt;code class="dockerfile hljs"&gt;&lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; --from=builder /go/bin/dlv /usr/bin/&lt;/span&gt; ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This would ensure that at the time of running the binary in the container as a part of the operator deployment, we can run it using &lt;code&gt;dlv&lt;/code&gt; and bind the debug stub inside the container to a port that can later be port-forwarded.&lt;/p&gt; &lt;p&gt;The final Dockerfile would be as follows:&lt;/p&gt; &lt;pre class="part" data-endline="109" data-position="6613" data-startline="97"&gt; &lt;code class="dockerfile hljs"&gt;&lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... AS builder &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; go install -mod=&lt;span class="hljs-built_in"&gt;readonly&lt;/span&gt; github.com/go-delve/delve/cmd/dlv@latest&lt;/span&gt; &lt;span class="hljs-keyword"&gt;WORKDIR&lt;/span&gt;&lt;span class="language-bash"&gt; /go/src/github.com/openshift/cluster-kube-apiserver-operator&lt;/span&gt; &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; . .&lt;/span&gt; &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GO_PACKAGE github.com/openshift/cluster-kube-apiserver-operator &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GCFLAGS &lt;span class="hljs-string"&gt;"all=-N -l"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; make build --warn-undefined-variables&lt;/span&gt; &lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; --from=builder /go/bin/dlv /usr/bin/&lt;/span&gt; ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The same can be done for any other operator including ones which are not cluster operators. The only necessity is to build the Go binary with &lt;code&gt;-l&lt;/code&gt; and &lt;code&gt;-N&lt;/code&gt; to ensure that the linker keeps the symbols for helping to debug later. The size of the debug binary and hence the debug image could be more than the stripped binary we ship in production operators.&lt;/p&gt; &lt;p&gt;After obtaining the newly modified Dockerfile at &lt;code&gt;Dockerfile.debug&lt;/code&gt;, we can build and push it to the registry using:&lt;/p&gt; &lt;pre class="part" data-endline="129" data-position="7468" data-startline="114"&gt; &lt;code class="bash hljs"&gt;$ podman build -t quay.io/&lt;USERNAME&gt;/&lt;REPO&gt;:&lt;ANY_TAG&gt; -f Dockerfile.debug . [1/2] STEP 1/7: ... --- [2/2] COMMIT quay.io/swghosh/cluster-kube-apiserver-operator:debug --&gt; b18f722bd49 Successfully tagged quay.io/swghosh/cluster-kube-apiserver-operator:debug b18f722bd49ad82b8763917800eb0481ef0135b6b1f619973a6fb7c144a09cef $ podman push quay.io/&lt;USERNAME&gt;/&lt;REPO&gt;:&lt;ANY_TAG&gt; --- Copying blob 7c33fa50bff3 &lt;span class="hljs-keyword"&gt;done&lt;/span&gt; Copying config b18f722bd4 &lt;span class="hljs-keyword"&gt;done&lt;/span&gt; Writing manifest to image destination Storing signatures &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next step would be to patch the deployment of the running operator to use the this newly prepared image and alter the container args of the same to run using &lt;code&gt;dlv&lt;/code&gt;.&lt;/p&gt; &lt;pre class="part" data-endline="150" data-position="8151" data-startline="133"&gt; &lt;code class="bash hljs"&gt;$ oc edit deployment kube-apiserver-operator -n openshift-kube-apiserver-operator &lt;span class="hljs-comment"&gt;# Change the spec.template.containers[0].args,command to use dlv&lt;/span&gt; - args: - --listen=:40000 - --headless=&lt;span class="hljs-literal"&gt;true&lt;/span&gt; - --api-version=2 - --accept-multiclient - &lt;span class="hljs-built_in"&gt;exec&lt;/span&gt; - /usr/bin/cluster-kube-apiserver-operator - -- - operator - --config=/var/run/configmaps/config/config.yaml &lt;span class="hljs-built_in"&gt;command&lt;/span&gt;: - /usr/bin/dlv &lt;span class="hljs-comment"&gt;# Change the spec.template.containers[0].image&lt;/span&gt; image: quay.io/swghosh/cluster-kube-apiserver-operator:debug &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The dlv binary needs to be run in the container which would execute the built Golang binary of the operator. For any operator, the patch would finally to run the command: &lt;code&gt;/usr/bin/dlv --listen=:40000 --headless=true --api-version=2 --accept-multiclient exec /usr/bin/&lt;operator_binary&gt; -- &lt;other_operator_args&gt;&lt;/code&gt;. Headless and listen arguments for dlv are required to enable the dlv debugger to run as a stub in headless mode and bind it to a container port which we can access later. Once edited, we can save the deployment and close the editor for the new operator pod to take effect.&lt;/p&gt; &lt;p&gt;Verify that the new operator pod is running after the manifest change as follows:&lt;/p&gt; &lt;pre class="part" data-endline="160" data-position="9410" data-startline="156"&gt; &lt;code class="bash language-bash"&gt;$ oc get pods -n openshift-kube-apiserver-operator NAME READY STATUS RESTARTS AGE kube-apiserver-operator-86c5fc45cd-rr695 1/1 Running 0 29s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the pod is in running state, port-forward 40000 port from the container which is the dlv debug port (used before as a part of &lt;code&gt;--listen&lt;/code&gt;). This would enable the traffic at localhost:40000 to be forwarded to 40000 port bound to dlv process inside the container. Re-run the command if the connection for the port-forward times out.&lt;/p&gt; &lt;pre class="part" data-endline="168" data-position="9962" data-startline="164"&gt; &lt;code class="bash language-bash"&gt;$ oc port-forward pod/kube-apiserver-operator-65bd9656cc-jdvjr 40000 -n openshift-kube-apiserver-operator Forwarding from 127.0.0.1:40000 -&gt; 40000 Forwarding from [::1]:40000 -&gt; 40000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, this operator can be debugged from VS Code or using dlv from the command line and connecting to localhost:40000. To debug it using VS Code, select the remote attach debugger option in &lt;code&gt;Run &gt; Add Configuration &gt; Go: Connect to Server&lt;/code&gt; (as shown in Figure 1). This action will allow remote Go debugging.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/1_2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/1_2.png?itok=Y5pzyB2G" width="600" height="215" alt="The VScode debug target window." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: In VS Code, this debug target is selected "Go: Connect to Server" to allow remote Go debugging.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, connect to the localhost (as shown in Figure 2).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/2_5.png?itok=wsv1D5SL" width="600" height="215" alt="Setting the VS code debug target to remote host." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2. In VS Code, set the debug target "localhost" to the remote host.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Then, set the port to 40000 (as shown in Figure 3).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/3_7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/3_7.png?itok=M5dQ9d-q" width="600" height="257" alt="The VS code debug window for setting the remote port." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3. In the VS Code window, set the debug port to 40000.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Finally, the &lt;code&gt;launch.json&lt;/code&gt; in the &lt;code&gt;.vscode&lt;/code&gt; directory of the local source directory would contain something similar to what is shown in Figure 4. The contents of &lt;code&gt;launch.json&lt;/code&gt; will be auto-generated with the necessary configurations provided by the remote host and port set in the previous steps.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/4_4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/4_4.png?itok=rpnmBKjF" width="600" height="257" alt="Contents of the launch.json in the VScode window for remote Go debugging." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4. The contents of the launch.json after setting the debug host and port should be similar.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After the launch.json is setup with the necessary details, you can start the &lt;code&gt;Debug &gt; Connect to server&lt;/code&gt; target (as shown in Figure 5). Start live debugging of the Go binary running through delve inside the cluster.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/5_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/5_6.png?itok=MfMUQNGk" width="600" height="121" alt="Shows where to start live degugging in the VS code window from the debug explorer." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5. From the debug explorer in VS code, select the "Connect to server" debug target to start live debugging.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now you can use breakpoints, watch, check the current call stack, and do much more with your operator all while it is running live in the cluster.&lt;/p&gt; &lt;h2&gt;Running remote delve simplifies debugging operators&lt;/h2&gt; &lt;p&gt;We have illustrated how to simplify debugging operators by live debugging using dlv remotely. You can say goodbye to eerie print statements. If you have questions, please comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/24/how-debug-openshift-operators-live-cluster-using-dlv" title="How to debug OpenShift operators on a live cluster using dlv"&gt;How to debug OpenShift operators on a live cluster using dlv&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Swarup Ghosh</dc:creator><dc:date>2023-04-24T18:00:00Z</dc:date></entry><entry><title type="html">How to Trace requests with RestEasy</title><link rel="alternate" href="https://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-trace-requests-with-resteasy/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-trace-requests-with-resteasy/</id><updated>2023-04-20T11:30:45Z</updated><content type="html">When working with RESTful web services, it is often useful to be able to trace requests to better understand what is happening. Fortunately, this is relatively easy to do with WildFly and Resteasy. In this article, we will look at how to configure tracing for REST requests using an enhancement available in WildFly 28. There ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to build RHEL images for edge deployments</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/20/how-build-rhel-images-edge-deployments" /><author><name>Chris Santiago</name></author><id>26ea5a25-7501-4211-a188-9e6117aafd7e</id><updated>2023-04-20T07:00:00Z</updated><published>2023-04-20T07:00:00Z</published><summary type="html">&lt;p&gt;As &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt; infrastructure scales outside the data center into remote locations, small-factor devices such as IoT, POS, and sensors that have &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; images, need a way to be updated at scale.&lt;/p&gt; &lt;p&gt;The rpm-ostree core premise is that by default, the updates should base on a whole base image that is created and tested offline, and once ready deployed everywhere into the remote locations, overriding the previous image and lowering the risks of patching at scale.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-03-03_08-55-18.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-03-03_08-55-18.png?itok=soYJAaEV" width="533" height="354" alt="An illustration of the image builder." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The image builder.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We know many people are working in secure edge environments and need the ability to create and lifecycle manage operating systems. In order to help with this we have created a way to build, host and manage these images using the &lt;a href="https://developers.redhat.com/topics/ansible-automation-applications-and-services"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article will cover the &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/collections_using.html"&gt;Ansible collection&lt;/a&gt; for management of the&lt;a href="https://www.osbuild.org/documentation/#composer"&gt; osbuild composer&lt;/a&gt; to build&lt;a href="https://rpm-ostree.readthedocs.io/en/latest/"&gt; rpm-ostree&lt;/a&gt; based images for Fedora, &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;, and CentOS Stream. This collection has roles to build an osbuild server, an apache httpd server to host images, and a role to build installer images and rpm-ostree updates.&lt;/p&gt; &lt;h2&gt;2 roles of the infra.osbuild collection&lt;/h2&gt; &lt;p&gt;There are two roles that are part of the infra.osbuild collection:&lt;/p&gt; &lt;h3&gt;1. The infra.osbuild.setup_server role&lt;/h3&gt; &lt;p&gt;The setup_server role checks to see what type of OS the remote system is running, ostree-based or non ostree-based. A remote system running an OS based on ostree will need to have packages already installed via a previous commit or with the initial install to continue. Non-ostree based hosts will have all the necessary packages installed.&lt;/p&gt; &lt;p&gt;At the same time the setup_server role also ensures all necessary services are enabled and started. Lastly it adds support for rpm custom repositories for adding custom packages to images.&lt;/p&gt; &lt;h3&gt;2. The infra.osbuild.builder role&lt;/h3&gt; &lt;p&gt;This builder role creates a blueprint based on information provided by the playbook variables such as packages, user info, and compose type. A rpm-ostree repository is initialized for the blueprint name to handle commits and upgrades. The builder role creates an image based on the previously created blueprint. Lastly it creates a kickstart file which supports an optional auto registration to be used on a system.&lt;/p&gt; &lt;h2&gt;How to build images for edge deployment&lt;/h2&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; To test this collection you will need either a RHEL, CentOS Stream, or Fedora system.&lt;/p&gt; &lt;p&gt;Install the infra.osbuild collection using the &lt;a href="https://docs.ansible.com/ansible/latest/galaxy/user_guide.html#installing-collections"&gt;ansible-galaxy&lt;/a&gt; command as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-galaxy collection install git+https://github.com/redhat-cop/infra.osbuild&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once installed we will make an empty directory to store the example playbooks and inventory file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir osbuild_example&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an &lt;a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html"&gt;inventory file&lt;/a&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;touch inventory.ini&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Inside the inventory file, create a group named &lt;code&gt;all&lt;/code&gt; with the remote systems IP address underneath.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;[all] &lt;Host IP&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that we have an inventory file and a remote system to point to. Let’s take a look into the setup_role. As explained above, this role simply sets up all the necessary packages and services to get osbuild up and running. If you plan on using a custom rpm repository to add custom packages that you would like to make available to osbuild then we need to add some configuration otherwise we can use the role as is.&lt;/p&gt; &lt;p&gt;Create a playbook named &lt;code&gt;osbuild_setup_server.yaml&lt;/code&gt; with the sample playbook as its contents as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;--- - name: Run osbuild_server role hosts: all become: true tasks: - name: Run the role ansible.builtin.import_role: name: infra.osbuild.setup_server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For adding a custom rpm repository, we can pass a list to the &lt;code&gt;setup_server_custom_repos&lt;/code&gt; role variable. Each list entry is a&lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html"&gt; YAML dictionary&lt;/a&gt; type and has the following attributes:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;code&gt;repo_name&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;base_url&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;type&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;check_ssl&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;check_gpg&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;rhsm&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;state&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If you wanted support for custom rpm repositories your playbook should something like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;--- - name: Run osbuild_server role hosts: all become: true vars: setup_server_custom_repos: - name: EPEL Everything base_url: "https://dl.fedoraproject.org/pub/epel/{{ hostvars[inventory_hostname].ansible_distribution_major_version }}/Everything/x86_64/" type: yum-baseurl check_ssl: true check_gpg: true state: present - name: My company custom repo base_url: "https://repo.example.com/company_repo/x86_64/" type: yum-baseurl tasks: - name: Run the role ansible.builtin.import_role: name: infra.osbuild.setup_server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that we have the osbuild_setup_server file completed, we can run the playbook using this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-playbook -i inventory.ini –ask-become –ask-pass playbooks/osbuild_setup_server.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We will run the playbook with &lt;code&gt;–ask-become&lt;/code&gt; and &lt;code&gt;–ask-pass&lt;/code&gt; flags to provide basic authentication, or if you want to set up proper authentication with ssh keys and proper user sudo management.&lt;/p&gt; &lt;p&gt;Once the playbook has finished running then the osbuild server is ready for us to start building images.&lt;/p&gt; &lt;p&gt;As explained  in the infra.osbuild.builder section, there are variables that are needed to create a blueprint. You can refer to the &lt;a href="https://github.com/redhat-cop/infra.osbuild/tree/main/roles/builder"&gt;full list and their explanations&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let’s create another playbook called &lt;code&gt;osbuild_builder.yaml&lt;/code&gt; with the sample playbook as its contents as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;--- - name: Run osbuild_builder role become: true hosts: all vars: builder_compose_type: edge-commit builder_blueprint_name: mybuild builder_pub_key: ~/.ssh/id_rsa.pub builder_compose_pkgs: - vim-enhanced - httpd - ansible-core - tmux builder_compose_customizations: user: name: "testuser" description: "test user" password: "testpassword" key: "{{ builder_pub_key }}" groups: '["users", "wheel"]' tasks: - name: Run the role ansible.builtin.import_role: name: infra.osbuild.builder&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you would like to have your image automatically register with Ansible Automation Platform, add &lt;code&gt;builder_aap_url&lt;/code&gt;, &lt;code&gt;builder_set_hostname&lt;/code&gt;, &lt;code&gt;builder_aap_ks_user&lt;/code&gt; and &lt;code&gt;builder_aap_ks_password&lt;/code&gt; underneath the vars section in the &lt;code&gt;osbuild_builder.yaml&lt;/code&gt; playbook.&lt;/p&gt; &lt;p&gt;Once you have finished writing your playbook run the &lt;code&gt;osbuild_builder.yml&lt;/code&gt; playbook to begin building an image.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-playbook -i inventory.ini –ask-become –ask-pass playbooks/osbuild_builder.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After the playbook finishes, log in into your remote system &lt;code&gt;http://&lt;ip_addr&gt;/&lt;blueprint_name&gt;/&lt;/code&gt; to see the hosted repo and kickstart file that can be used to provision a new system.&lt;/p&gt; &lt;h2&gt;Find more resources&lt;/h2&gt; &lt;p&gt;In summary, infra.osbuild is an easy-to-use solution for creating customizable images. Ansible validated content for infrastructure osbuild collection, will let you automate the provisioning and configuration of the required osbuild components and build a RHEL image for your edge deployments.&lt;/p&gt; &lt;p&gt;If you want to learn more about edge &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt;, here are a few suggestions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Explore resources at &lt;a href="https://www.ansible.com/use-cases/edge"&gt;Using Red Hat Ansible Automation Platform for edge computing&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download the &lt;a href="https://developers.redhat.com/e-books/automation-at-the-edge"&gt;Automation at the edge e-book&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;For additional use cases such as industrial protocol integration, read the article, &lt;a href="https://developers.redhat.com/articles/2023/01/10/automate-devices-using-ansible-cip"&gt;How to automate devices using the Ansible CIP collection&lt;/a&gt;. &lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/20/how-build-rhel-images-edge-deployments" title="How to build RHEL images for edge deployments"&gt;How to build RHEL images for edge deployments&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Chris Santiago</dc:creator><dc:date>2023-04-20T07:00:00Z</dc:date></entry><entry><title type="html">WildFly 28 is released!</title><link rel="alternate" href="https://wildfly.org//news/2023/04/20/WildFly28-Released/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2023/04/20/WildFly28-Released/</id><updated>2023-04-20T00:00:00Z</updated><content type="html">I’m pleased to announce that the new WildFly and WildFly Preview 28.0.0.Final releases are available for download at . NEW AND NOTABLE Observability The biggest changes in WildFly 28 relate to the observability space. * The micrometer subsystem has been , bringing support. As part of this work, we’ve added support for . The micrometer subsystem was first introduced in WildFly Preview in WildFly 27. Note that the subsystem has been updated from what was in WildFly Preview 27 to switch to pushing metric data via OTLP to a remote collector, instead of supporting polling of data on the WildFly server’s management interface. (Server and JVM metrics can still be pulled from the management endpoint if the is configured.) * We’ve also added support for via a . * We’ve removed support for MicroProfile Metrics, except for a that’s been kept to facilitate configuration migration. MicroProfile Metrics users are encouraged to use the new micrometer subsystem. * We’ve removed support for MicroProfile OpenTracing, except for a that’s been kept to facilitate configuration migration. MicroProfile OpenTracing users are encouraged to use the new microprofile-telemetry subsystem, or the opentelemetry subsystem upon which it is based. MicroProfile Besides the changes in the observability space noted above, there are a couple of important changes in WildFly 28’s MicroProfile support: * We’ve for via new microprofile-lra-coordinator and microprofile-lra-participant subsystems. * Except for MicroProfile Metrics and OpenTracing, which have been removed, we’ve updated our support for the other MicroProfile Platform specifications to the versions. Because we no longer support MicroProfile Metrics, WildFly 28 cannot claim to be a compatible implementation of the MicroProfile 6.0 specification. However, WildFly’s MicroProfile support includes implementations of the following specifications in our "full" (e.g. standalone-full.xml) and "default" (e.g standalone.xml) configurations as well as our "microprofile" configurations (e.g. standalone-microprofile.xml): MicroProfile Technology WildFly Full/Default Configurations WildFly MicroProfile Configuration MicroProfile Config 3.0 X X MicroProfile Fault Tolerance 4.0  —  X MicroProfile Health 4.0  —  X MicroProfile JWT Authentication 2.1 X X MicroProfile LRA 2.0  —  X MicroProfile OpenAPI 3.1  —  X MicroProfile Open Telemetry 1.0  —  X MicroProfile Reactive Messaging 3.0  —   —  MicroProfile Reactive Streams Operators 3.0  —   —  MicroProfile Rest Client 3.0 X X Provisioning * We’ve added a new to make it easy to provision a server based on the new introduced in EE 10. * Related to this we’ve introduced new and Galleon layers. These layers allow a more tailored configuration compared to the existing ee and web-server layers. Also, separate from WildFly itself, to help users in their migration from Jakarta EE 8 to EE 10 we’ve introduced a separate that provides a new Galleon feature pack. The wildfly-deployment-transformer-feature-pack allows you to integrate into a standard WildFly installation the EE 8 to EE 9 deployment transformation functionality that we’ve since its first release. See the for documentation on how to use this new feature pack. Quickstarts * Eduardo Martins and the teams working on server provisioning and cloud have done a significant enhancement to the WildFly quickstarts to . * We’ve also added a that . Other Treats * The server kernel team has added support for . is a nice alternative to using CLI scripts to tailor a stock configuration for a particular environment, as there is no need start a CLI process to apply the customization. This makes it well suited to workflows like . * The clustering team has added support for . * The clustering and web teams have added support for . * The RESTEasy team has added support for . * The messaging-activemq subsystem now supports . * When you use OIDC, the security team has added support for . * The web team has added for Undertow listeners. * We’ve updated Hibernate ORM from the ORM 6.1 release to 6.2.1. JAKARTA EE 10 SUPPORT WildFly 28 is a compatible implementation of the EE 10 as well as the and the new . WildFly is EE 10 compatible when running on both Java SE 11 and Java SE 17. Evidence supporting our certification is available in the repository on GitHub: * Jakarta EE 10 Full Platform * * * Jakarta EE 10 Web Profile * * * Jakarta EE 10 Core Profile * * JAVA SE SUPPORT Our recommendation is that you run WildFly on the most recent long-term support Java SE release, i.e. on SE 17 for WildFly 28. While we do do some testing of WildFly on JDK 20, we do considerably more testing of WildFly itself on the LTS JDKs, and we make no attempt to ensure the projects producing the various libraries we integrate are testing their libraries on anything other than JDK 11 or 17. WildFly 28 also is heavily tested and runs well on Java 11. We plan to continue to support Java 11 at least through WildFly 29, and likely beyond. We do, however, anticipate removing support for SE 17 sometime in the next 12 to 18 months. While we recommend using an LTS JDK release, I do believe WildFly runs well on JDK 20. By runs well, I mean the main WildFly testsuite runs with no more than a few failures in areas not expected to be commonly used. We want developers who are trying to evaluate what a newer JVM means for their applications to be able to look to WildFly as a useful development platform. Please note that WildFly runs on Java 11 and later in classpath mode. KNOWN ISSUES SPRING AND RESTEASY SPRING In WildFly 27, pending the final release of Spring 6, RESTEasy Spring support was removed from standard WildFly, and was only provided with WildFly Preview. With WildFly 28 we have reintroduced RESTEasy Spring support to standard WildFly. However, we’ve learned of a in WildFly 28 that will prevent Spring deployments, including those using RESTEasy Spring, from working. Until this is resolved in WildFly 28.0.1, users can work around this issue by to their deployment that declares a dependency on the org.jboss.vfs module. RELEASE NOTES The full release notes for the release are in the . Issues fixed in the underlying and releases are listed in the WildFly Core JIRA. Please try it out and give us your feedback, while we get to work on WildFly 29! Best regards, Brian</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title>OpenJDK 8u372 to feature cgroup v2 support</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/19/openjdk-8u372-feature-cgroup-v2-support" /><author><name>Severin Gehwolf</name></author><id>ed0cf383-c604-47bf-b79e-5edc5248c27f</id><updated>2023-04-19T07:00:00Z</updated><published>2023-04-19T07:00:00Z</published><summary type="html">&lt;p&gt;The control group (cgroup) pseudo filesystem is the key feature enabling resource quotas on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;containers&lt;/a&gt; deployed on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. The cgroups filesystem is a &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; kernel feature and comes in one of three forms, depending on the hosts' configuration:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;cgroup v2, or unified hierarchy&lt;/li&gt; &lt;li&gt;cgroup v1, or legacy hierarchy&lt;/li&gt; &lt;li&gt;hybrid (basically cgroup v1, but some system services use cgroup v2).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The state of the art is cgroup v2. With the releases of &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4.12&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux 9&lt;/a&gt;, which both feature cgroup v2, it becomes increasingly likely that OpenJDK 8 running in containers runs on a cgroup v2-enabled Linux kernel. Cgroup v1, the current predominant configuration, will become increasingly less frequent in practice as time moves forward.&lt;/p&gt; &lt;h2&gt;OpenJDK 8u372: cgroup v1 and v2 support&lt;/h2&gt; &lt;p&gt;With the release of OpenJDK 8u372 in April 2023, the cgroup v2 support patches present in later &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="d5069613-6b53-421b-9e45-bf8cf43625de" href="https://developers.redhat.com/articles/2022/04/19/java-17-whats-new-openjdks-container-awareness" title="Java 17: What’s new in OpenJDK's container awareness"&gt;JDK releases&lt;/a&gt; have been backported to OpenJDK 8. 30 patches in total have been backported so as to bring this feature to OpenJDK 8u. It was added to OpenJDK 8u, a very mature and stable JDK release, under the enhancement exception rule of adapting to new hardware and operating environments.&lt;/p&gt; &lt;p&gt;Version 8u372 and later will detect the cgroup version in use, v1 or v2, on the host system. Once it detects the version, it looks up the set resource limits via the pseudo filesystem hierarchy and will size its internal resources accordingly. This will ensure that OpenJDK 8 will comply to the set resource limits of containers on cgroup v2 systems, a feature OpenJDK users have grown accustomed to since it was first &lt;a href="https://bugs.openjdk.org/browse/JDK-8146115"&gt;brought to OpenJDK 8 with the 8u192&lt;/a&gt; release.&lt;/p&gt; &lt;h2&gt;How to see which cgroup version OpenJDK 8u detected&lt;/h2&gt; &lt;p&gt;One easy way to see which cgroup version, if any, OpenJDK 8u detected is by using the &lt;code&gt;-XshowSettings:system&lt;/code&gt; &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; launcher switch. Example:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[root@357fec96b37e /]# /opt/jdk8u372/bin/java -XshowSettings:system -version Operating System Metrics: Provider: cgroupv2 Effective CPU Count: 3 CPU Period: 100000us CPU Quota: 300000us CPU Shares: -1 List of Processors: N/A List of Effective Processors, 4 total: 0 1 2 3 List of Memory Nodes: N/A List of Available Memory Nodes, 1 total: 0 Memory Limit: 500.00M Memory Soft Limit: 0.00K Memory &amp; Swap Limit: 500.00M openjdk version "1.8.0_372-beta" OpenJDK Runtime Environment (Temurin)(build 1.8.0_372-beta-202303201451-b05) OpenJDK 64-Bit Server VM (Temurin)(build 25.372-b05, mixed mode)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Another way is to use the &lt;code&gt;-XX:+UnlockDiagnosticVMOptions -XX:+PrintContainerInfo&lt;/code&gt; JVM switches in order to see the details of which files are being looked up internally:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[root@357fec96b37e /]# /opt/jdk8u372/bin/java -XX:+UnlockDiagnosticVMOptions -XX:+PrintContainerInfo -version OSContainer::init: Initializing Container Support Detected cgroups v2 unified hierarchy Path to /cpu.max is /sys/fs/cgroup//cpu.max Raw value for CPU quota is: 300000 CPU Quota is: 300000 Path to /cpu.max is /sys/fs/cgroup//cpu.max CPU Period is: 100000 Path to /cpu.weight is /sys/fs/cgroup//cpu.weight Raw value for CPU Shares is: 100 CPU Shares is: -1 CPU Quota count based on quota/period: 3 OSContainer::active_processor_count: 3 CgroupSubsystem::active_processor_count (cached): 3 total physical memory: 5033832448 Path to /memory.max is /sys/fs/cgroup//memory.max Raw value for memory limit is: 524288000 Memory Limit is: 524288000 total container memory: 524288000 total container memory: 524288000 CgroupSubsystem::active_processor_count (cached): 3 Path to /cpu.max is /sys/fs/cgroup//cpu.max Raw value for CPU quota is: 300000 CPU Quota is: 300000 Path to /cpu.max is /sys/fs/cgroup//cpu.max CPU Period is: 100000 Path to /cpu.weight is /sys/fs/cgroup//cpu.weight Raw value for CPU Shares is: 100 CPU Shares is: -1 CPU Quota count based on quota/period: 3 OSContainer::active_processor_count: 3 openjdk version "1.8.0_372-beta" OpenJDK Runtime Environment (Temurin)(build 1.8.0_372-beta-202303201451-b05) OpenJDK 64-Bit Server VM (Temurin)(build 25.372-b05, mixed mode)&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;OpenJDK 8u362 and older: cgroup v1 only&lt;/h2&gt; &lt;p&gt;Older releases of OpenJDK 8u will only be able to detect cgroup v1 systems. Security considerations aside, it's highly encouraged to upgrade to a later release if you are running your containers on recent cloud infrastructure such as OpenShift 4.12. For example, for a 8u362 build of OpenJDK on a cgroup's v2 system, it would look like as if the container detection failed (i.e., no metrics):&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ./jdk8u362-b09/bin/java -XshowSettings:system -version Operating System Metrics: No metrics available for this platform openjdk version "1.8.0_362" OpenJDK Runtime Environment (Temurin)(build 1.8.0_362-b09) OpenJDK 64-Bit Server VM (Temurin)(build 25.362-b09, mixed mode) &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Onward to JDK 21&lt;/h2&gt; &lt;p&gt;While it's important to support new computing environments in older JDK releases, the next OpenJDK LTS release, OpenJDK 21, is around the corner and is scheduled to be released in September 2023. JDK 21 includes many &lt;a href="https://openjdk.org/jeps/387"&gt;more improvements&lt;/a&gt; for a better container and cloud experience. So if you are thinking of writing new Java applications, consider targeting JDK 21, and perhaps using &lt;a href="https://www.quarkus.io"&gt;Quarkus&lt;/a&gt; for the best cloud native experience!&lt;/p&gt; &lt;p&gt;Looking for more resources? &lt;a href="https://developers.redhat.com/java"&gt;Explore all things Java on Red Hat Developer.&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/19/openjdk-8u372-feature-cgroup-v2-support" title="OpenJDK 8u372 to feature cgroup v2 support"&gt;OpenJDK 8u372 to feature cgroup v2 support&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Severin Gehwolf</dc:creator><dc:date>2023-04-19T07:00:00Z</dc:date></entry><entry><title>New Features for Qute Templating Engine Support in Quarkus Tools for Visual Studio Code 1.13.0</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/vscode-quarkus-1.13.0-released/&#xA;            " /><author><name>Jessica He (https://twitter.com/JessicaJjhe)</name></author><id>https://quarkus.io/blog/vscode-quarkus-1.13.0-released/</id><updated>2023-04-19T00:00:00Z</updated><published>2023-04-19T00:00:00Z</published><summary type="html">Quarkus Tools for Visual Studio Code 1.13.0 has been released on the VS Code Marketplace and Open VSX. This release focuses on Qute Templating Engine Support by introducing support for more sections and improving template validation. New Features Notable Qute features included in Quarkus Tools for Visual Studio Code 1.13.0...</summary><dc:creator>Jessica He (https://twitter.com/JessicaJjhe)</dc:creator><dc:date>2023-04-19T00:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.36.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/04/kogito-1-36-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/04/kogito-1-36-0-released.html</id><updated>2023-04-18T13:12:34Z</updated><content type="html">We are glad to announce that the Kogito 1.36.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Added support for CloudEvents to Serverless Workflow Knative custom function * Implement action condition on Serverless Workflow * Job Service embedded Quarkus extension * Added Workflow executor. This allows  embedded execution of workflows in a standard JVM.  * Added first draft of Workflow definition fluent API. This allows programmatically defining workflows.  * Added support for OnOverflow annotation. This allows users to control event publisher overflow policy through application properties.  * Fixed bug that prevents using JQ expressions in arrays.   For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.27.0 artifacts are available at the . A detailed changelog for 1.36.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry></feed>
